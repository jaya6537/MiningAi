from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
from langchain import LLMChain, PromptTemplate
from langchain.memory import ConversationBufferMemory
import sys
import os
import gradio as gr
import nltk
nltk.download('punkt')
from langdetect import detect
from googletrans import Translator
from nltk.tokenize import sent_tokenize
from googletrans import Translator
from IPython.display import Markdown, display


def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)

    index.save_to_disk('index.json')

    return index


def post_process_response(response_text):
    # Tokenize the response into sentences
    sentences = sent_tokenize(response_text)

    # Capitalize the first letter of the first sentence
    sentences[0] = sentences[0].capitalize()

    # Join the sentences back into a single string
    processed_response = ' '.join(sentences)

    return processed_response


# Create a translator instance
translator = Translator()

# Define a function for language translation
def translate_response(response_text, target_language):
    translation = translator.translate(response_text, dest=target_language)
    return translation.text

def get_user_language(input_text):
    try:
        detected_language = detect(input_text)
    except:
        # Handle language detection error, e.g., if the input is too short
        detected_language = 'en'  # Default to English if language detection fails
    return detected_language


def ask_ai(query):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    
    # Detect the user's input language
    user_language = get_user_language(query)
    
    # Specify the detected language as the target language for translation
    response = index.query(query)
    
    # Post-process the response for better readability
    processed_response = post_process_response(response.response)
    
    # Translate the response to the detected user language
    translated_response = translate_response(processed_response, user_language)
    
    return f"Response: *{translated_response}*"

# Example usage:
# To get a response in French, set target_language='fr'
# To get a response in Spanish, set target_language='es'



OPENAI_API_KEY="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"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

construct_index("chatbotdata/mines")

iface = gr.Interface(
    fn=ask_ai,
    inputs="text",
    outputs=gr.outputs.Textbox(),
    layout="vertical",
    title="Ask AI",
    description="Ask the AI any question.",
)

iface.launch(share=True)
